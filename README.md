# ğŸ‡»ğŸ‡³ Qwen3-VL Finetuning on VietTravelVQA

Fine-tune **Qwen3-VL** (Vision-Language Model) trÃªn bá»™ dá»¯ liá»‡u **VietTravelVQA** - Dataset VQA vá» du lá»‹ch Viá»‡t Nam.

Sá»­ dá»¥ng **[Unsloth](https://unsloth.ai)** Ä‘á»ƒ tá»‘i Æ°u training vá»›i **4-bit quantization** vÃ  **LoRA**.

## âœ¨ Features

- ğŸ¦¥ **Unsloth Optimization** - Training nhanh hÆ¡n 2x, tiáº¿t kiá»‡m VRAM
- ğŸ“¦ **4-bit Quantization** - Cháº¡y Ä‘Æ°á»£c trÃªn GPU 8GB+
- ğŸ”§ **LoRA Finetuning** - Chá»‰ train adapter, khÃ´ng cáº§n full model
- ğŸŒ **HuggingFace Integration** - Load data tá»« `Qhuy204/VQA`
- ğŸ–¼ï¸ **Vision-Language** - Há»— trá»£ cÃ¢u há»i vá» hÃ¬nh áº£nh

## ğŸš€ Quick Start

### Installation

```bash
# Clone repo
git clone https://github.com/Qhuy204/VQA.git
cd VQA

# Install dependencies
pip install unsloth
pip install -r requirements.txt
```

### Training

```bash
# Load data tá»« HuggingFace (Qhuy204/VQA)
python finetune_qwen3vl.py --hf_dataset Qhuy204/VQA

# Hoáº·c tá»« local files
python finetune_qwen3vl.py \
    --train_file ./VietTravelVQA/viettravelvqa_train.json \
    --image_dir ./VietTravelVQA/images

# Test nhanh vá»›i 50 samples
python finetune_qwen3vl.py --hf_dataset Qhuy204/VQA --max_samples 50 --max_steps 30
```

### Inference

```bash
python inference.py \
    --model_path outputs/qwen3vl-viettravelvqa/lora_model \
    --image path/to/image.jpg \
    --question "ÄÃ¢y lÃ  Ä‘á»‹a Ä‘iá»ƒm du lá»‹ch nÃ o?"
```

## ğŸ“Š Dataset

**VietTravelVQA** - Visual Question Answering vá» du lá»‹ch Viá»‡t Nam:

| Split | Images | QA Pairs |
|-------|--------|----------|
| Train | 1,124 | 5,620 |
| Test | 282 | 1,410 |

- ğŸ›ï¸ Di tÃ­ch lá»‹ch sá»­, vÄƒn hÃ³a
- ğŸ–ï¸ Äá»‹a Ä‘iá»ƒm du lá»‹ch
- ğŸœ áº¨m thá»±c Ä‘á»‹a phÆ°Æ¡ng
- ğŸ­ Lá»… há»™i truyá»n thá»‘ng

**Data Source:** [Qhuy204/VQA](https://huggingface.co/datasets/Qhuy204/VQA)

## âš™ï¸ Configuration

### Model Options

| Model | VRAM | HuggingFace |
|-------|------|-------------|
| 2B | ~6GB | `unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit` |
| **4B** | ~8GB | `unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit` |
| 8B | ~12GB | `unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit` |

### Training Arguments

| Argument | Default | Description |
|----------|---------|-------------|
| `--hf_dataset` | None | HuggingFace dataset (e.g., `Qhuy204/VQA`) |
| `--epochs` | 3 | Number of epochs |
| `--batch_size` | 2 | Batch size per GPU |
| `--lr` | 2e-4 | Learning rate |
| `--lora_r` | 16 | LoRA rank |

### A100 Optimized

```bash
python finetune_qwen3vl.py \
    --hf_dataset Qhuy204/VQA \
    --model_name unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit \
    --batch_size 4 \
    --lora_r 32
```

## ğŸ“ Project Structure

```
VQA/
â”œâ”€â”€ finetune_qwen3vl.py     # Main training script
â”œâ”€â”€ inference.py            # Inference script
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ training_config.yaml      # Default config
â”‚   â””â”€â”€ training_config_a100.yaml # A100 optimized
â””â”€â”€ README.md
```

## ğŸ“š References

- [Unsloth Qwen3-VL Guide](https://unsloth.ai/docs/models/qwen3-vl-how-to-run-and-fine-tune)
- [Qwen3-VL Model](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct)
- [VietTravelVQA Dataset](https://huggingface.co/datasets/Qhuy204/VQA)

## ğŸ“„ License

- **Code:** MIT License
- **Dataset:** CC BY 4.0
