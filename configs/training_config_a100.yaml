# Qwen3-VL Finetuning Configuration - Optimized for A100 GPU
# A100 40GB: batch_size=4, model 8B
# A100 80GB: batch_size=8, model 8B/32B

# =============================================================================
# Model Configuration
# =============================================================================
model:
  # A100 40GB: Use 8B model
  # A100 80GB: Can use 8B or even 32B
  name: "unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit"
  
  # Alternatives:
  # - 4B (lighter): unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit
  # - 32B (A100 80GB only): unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit
  
  # 4-bit quantization (still recommended for faster training)
  load_in_4bit: true
  
  # Unsloth optimized gradient checkpointing
  use_gradient_checkpointing: "unsloth"
  
  # Larger context for A100
  max_seq_length: 4096

# =============================================================================
# LoRA Configuration - Can use higher rank on A100
# =============================================================================
lora:
  r: 32                # Higher rank for better quality (16 â†’ 32)
  lora_alpha: 32       # Same as r
  lora_dropout: 0
  
  # Finetune all layers
  finetune_vision_layers: true
  finetune_language_layers: true
  finetune_attention_modules: true
  finetune_mlp_modules: true
  
  bias: "none"
  random_state: 3407
  use_rslora: false

# =============================================================================
# Training Arguments - Optimized for A100
# =============================================================================
training:
  output_dir: "./outputs/qwen3vl-viettravelvqa-a100"
  
  # Epochs
  num_train_epochs: 3
  max_steps: -1
  
  # A100 40GB Settings
  per_device_train_batch_size: 4    # 4x larger than consumer GPUs
  gradient_accumulation_steps: 4    
  # Effective batch size = 4 * 4 = 16
  
  # For A100 80GB, can increase to:
  # per_device_train_batch_size: 8
  # gradient_accumulation_steps: 2
  # Effective batch size = 8 * 2 = 16
  
  # Learning rate (can be slightly higher with larger batch)
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"     # Cosine is often better for longer training
  warmup_ratio: 0.1               # Use ratio instead of fixed steps
  weight_decay: 0.01
  
  # Optimizer - A100 supports bf16 well
  optim: "adamw_8bit"
  
  # Logging
  logging_steps: 10
  
  # Save more frequently with faster training
  save_steps: 200
  save_total_limit: 5
  
  # Evaluation
  eval_steps: 200
  
  # Max sequence length (A100 can handle longer)
  max_seq_length: 4096
  
  # Seed
  seed: 3407
  
  # WandB logging (recommended for production training)
  report_to: "wandb"
  run_name: "qwen3vl-viettravelvqa-a100"

# =============================================================================
# Data Configuration
# =============================================================================
data:
  train_file: "./VietTravelVQA/viettravelvqa_train.json"
  test_file: "./VietTravelVQA/viettravelvqa_test.json"
  image_dir: "./VietTravelVQA/images"
  
  # No limit for full training
  max_train_samples: null
  max_eval_samples: null

# =============================================================================
# A100 Specific Optimizations
# =============================================================================
performance:
  # Flash Attention 2 (highly recommended for A100)
  use_flash_attention_2: true
  
  # bf16 is native on A100
  bf16: true
  fp16: false
  
  # DataLoader optimization
  dataloader_num_workers: 8         # A100 systems often have many CPU cores
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2
  
  # Torch compile (optional, can speed up training)
  torch_compile: false  # Set to true for PyTorch 2.0+

# =============================================================================
# Export Options
# =============================================================================
export:
  save_lora: true
  save_merged_16bit: true  # A100 has enough memory to merge
  save_gguf: true
  gguf_quantization: "q8_0"  # Higher quality quantization
